{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Taylor Cheat Sheet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taylorfogarty/launch/blob/master/Taylor_Cheat_Sheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pU6dpX2mVWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, Ridge, LogisticRegression, RidgeClassifier \n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestClassifier\n",
        "from sklearn.kernel_ridge import KernelRidge \n",
        "from sklearn.pipeline import make_pipeline \n",
        "from sklearn.preprocessing import RobustScaler \n",
        "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
        "from sklearn.model_selection import KFold, cross_val_score, train_test_split \n",
        "from sklearn.metrics import mean_squared_error, f1_score, confusion_matrix, classification_report \n",
        "import xgboost as xgb \n",
        "import lightgbm as lgb\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from scipy import stats\n",
        "from scipy.stats import norm, skew\n",
        "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #limiting floats output to 3 decimal points"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9qt1lzEmhNB",
        "colab_type": "text"
      },
      "source": [
        "### Types of Regression Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UBkc2Fomdmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Linear Regression\n",
        "lm = LinearRegression()\n",
        "lm_fit = lm.fit(X, y)\n",
        "\n",
        "#Logistic Regression\n",
        "lr = LogisticRegression()\n",
        "lr_fit = lr.fit(X,y)\n",
        "\n",
        "#LASSO Regression\n",
        "lasso = Lasso(alpha=5)\n",
        "lasso_fit = lasso.fit(X,y)\n",
        "\n",
        "#Ridge Regression\n",
        "ridge = Ridge(alpha=5)\n",
        "ridge_fit = ridge.fit(X,y)\n",
        "\n",
        "#Logistic Ridge Regression\n",
        "ridge2 = RidgeClassifier(alpha=5)\n",
        "ridge_fit2 = ridge2.fit(X,y)\n",
        "\n",
        "#Elastic Net Regression\n",
        "ENet = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)\n",
        "ENet_fit = ENet.fit(X,y)\n",
        "\n",
        "#LASSO Regression (ROBUST)\n",
        "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
        "\n",
        "#Elastic Net Regression (ROBUST)\n",
        "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
        "\n",
        "#Kernel Ridge Regression\n",
        "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
        "KRR_fit = KRR.fit(X,y)\n",
        "\n",
        "#Gradient Boosting Regression (HUBER)\n",
        "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, \n",
        "                                   max_features='sqrt',min_samples_leaf=15, min_samples_split=10, \n",
        "                                   loss='huber', random_state =5)\n",
        "GBoost_fit = GBoost.fit(X,y)\n",
        "\n",
        "#XGBoosting\n",
        "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.05, max_depth=3, \n",
        "                             min_child_weight=1.7817, n_estimators=2200, reg_alpha=0.4640, reg_lambda=0.8571,\n",
        "                             subsample=0.5213, silent=1, random_state =7, nthread = -1)\n",
        "XGB_fit = model_xgb.fit(X,y)\n",
        "\n",
        "#LightGMB\n",
        "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,learning_rate=0.05, n_estimators=720,\n",
        "                              max_bin = 55, bagging_fraction = 0.8,bagging_freq = 5, feature_fraction = 0.2319,\n",
        "                              feature_fraction_seed=9, bagging_seed=9,min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
        "LGB_fit = model_lgb.fit(X,y)\n",
        "\n",
        "#Random Forest\n",
        "rand_for = RandomForestClassifier(n_estimators = 500, random_state = 40)\n",
        "rand_for_fit = rand_for.fit(X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyyhk2I7mocr",
        "colab_type": "text"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2jQt75DpPhv",
        "colab_type": "text"
      },
      "source": [
        "###Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20FGVSCOpRqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dummy Variables\n",
        "bank_train_d = pd.get_dummies(bank_train)\n",
        "#Drop Variable\n",
        "bank_train_d = bank_train_d.drop('duration',axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hgIQdKBnf07",
        "colab_type": "text"
      },
      "source": [
        "###Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksoUxlJimrpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Holdout\n",
        "x_train, x_test, y_train, y_test= train_test_split(x,y, test_size=0.10)\n",
        "#K-Folds\n",
        "n_folds = 5\n",
        "kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmromKPyniL4",
        "colab_type": "text"
      },
      "source": [
        "###Stacked Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlHzTwh4m9vx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Averaged Base Models Procedure \n",
        "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "        \n",
        "    # clones models\n",
        "    def fit(self, X, y):\n",
        "        self.models_ = [clone(x) for x in self.models]\n",
        "        \n",
        "        # training cloned models\n",
        "        for model in self.models_:\n",
        "            model.fit(X, y)\n",
        "\n",
        "        return self\n",
        "    \n",
        "    # averaging predictions of cloned models\n",
        "    def predict(self, X):\n",
        "        predictions = np.column_stack([\n",
        "            model.predict(X) for model in self.models_\n",
        "        ])\n",
        "        return np.mean(predictions, axis=1) \n",
        "\n",
        "averaged_models = AveragingModels(models = (lasso, KRR, ENet))\n",
        "\n",
        "#Meta-Model Procedure\n",
        "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
        "    def __init__(self, base_models, meta_model, n_folds=5):\n",
        "        self.base_models = base_models\n",
        "        self.meta_model = meta_model\n",
        "        self.n_folds = n_folds\n",
        "   \n",
        "    #fit the data on clones of the original models\n",
        "    def fit(self, X, y):\n",
        "        self.base_models_ = [list() for x in self.base_models]\n",
        "        self.meta_model_ = clone(self.meta_model)\n",
        "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
        "        \n",
        "        #train cloned base models then create out-of-fold predictions\n",
        "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
        "        for i, model in enumerate(self.base_models):\n",
        "            for train_index, holdout_index in kfold.split(X, y):\n",
        "                instance = clone(model)\n",
        "                self.base_models_[i].append(instance)\n",
        "                instance.fit(X[train_index], y[train_index])\n",
        "                y_pred = instance.predict(X[holdout_index])\n",
        "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
        "                \n",
        "        #train the cloned meta-model using the out-of-fold predictions\n",
        "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
        "        return self\n",
        "   \n",
        "    #computes predictions of base models and uses the averages as meta-features for meta-model's final predictions\n",
        "    def predict(self, X):\n",
        "        meta_features = np.column_stack([\n",
        "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
        "            for base_models in self.base_models_ ])\n",
        "        return self.meta_model_.predict(meta_features)\n",
        "      \n",
        "stacked_averaged_models = StackingAveragedModels(base_models = (lasso, KRR, ENet), meta_model = GBoost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-rEwjX-pEWO",
        "colab_type": "text"
      },
      "source": [
        "###Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04B-e20dpGKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Feature Importance\n",
        "feat_imp = pd.DataFrame(rand_for.feature_importances_, index=bank_train_num.columns)\n",
        "print(feat_imp)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}